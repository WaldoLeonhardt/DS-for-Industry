---
title: "Load and wrangle SONA data"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Load data

Start by extracting data out of the SONA files.
Build a data frame containing the filename, the speech text, president and year of speech.

```{r}
library(tidyverse)
#need to test github with this new comment!

# extract list of SONA files
# NOTE: that file name "2009_pre_elections_ Motlanthe.txt" has been corrected to remove the space before Motlanthe as this will cause issues later
#       when the president name needs to be assosiated with matrix entries
sona_files = list.files("data/")

# create empty data frame for SONA data
sona_data = data.frame(filename = as.character(), speech = as.character(), president = as.character())

# step through files and build sona_data
for(i in sona_files){
  this_file = paste0("data/", i)
  
  # extract president
  this_file_name = str_replace(i, ".txt", "") # remove .txt from file name
  
  this_president = str_sub(this_file_name, start = last(unlist(str_locate_all(this_file_name, "_")))+1, end = nchar(this_file_name))
  
  # extract speech text as single character string (can also read.table but the "seperator" causes problems)
  this_speech = readChar(this_file,
                         nchars = file.info(this_file)$size)
  
  # make data frame with metadata (filename contains year and pres) and speech
  this_sona = data.frame(filename = i, speech = this_speech, president = this_president, stringsAsFactors = FALSE)
  
  # add row to sona_data
  sona_data = rbind(sona_data, this_sona)
}

# add year to SONA data
sona_data$year = str_extract(sona_data$filename, "[0-9]{4}") # year is the first 4 numbers

head(sona_data)
```

Extract sentences

```{r}
library(tidytext)

# we want to predict sentences, so we need to first split into sentences
# use the speech attribute and replace with a sentence attribute, tokenized by sentences
sona_sentences = sona_data %>% unnest_tokens(sentence, speech, token = "sentences")

# convert to lower case
sona_sentences$sentence = str_to_lower(sona_sentences$sentence)

head(sona_sentences)
```

Extract words

```{r}
# add an ID variable for sentences
sona_sentences$sentence.id = rownames(sona_sentences) # use row number and assign to new ID variable

# use the sentence attribute and replace with a word attribute, tokenized by words
sona_words = sona_sentences %>% unnest_tokens(word, sentence, token = "words")

head(sona_words)
```

List the most frequent words used

```{r}
# view word frequencies
sona_words %>%
  count(word, sort = TRUE) %>%
  filter(rank(desc(n)) <= 30) # list top 30 ranked words
```

```{r}
# can also be done by the following
sona_words %>%
  group_by(word) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(30)
```

As expected, the most frequent words are stop-words. Let's remove those first.

```{r}
# remove stop-words
sona_words = sona_words %>%
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %>% # remove stop words
  #filter(!str_detect(word, "[[:punct:]]")) %>% # remove punctuation NOTE: this will remove all words containing any punctuation
  select(-filename)

head(sona_words, 30)
```

Look again at the words occuring the most

```{r}
# view word frequencies
sona_words %>%
  count(word, sort = TRUE) %>%
  filter(rank(desc(n)) <= 30) # list top 30 ranked words
```

Add count of words per sentance

```{r}
sona_words = sona_words %>%
  group_by(president, year, sentence.id, word) %>%
  summarise(count = n())

# list sentences with the most reoccuring words
sona_words %>%
  arrange(desc(count)) %>%
  head(30)
```

Reshape data in matrix form

```{r}
# build matrix of words per sentence.id
sona_matrix = sona_words %>%
  ungroup() %>% # remove grouping, otherwise complete and spread will use variables 'president' and 'year'
  select(sentence.id, word, count) %>%
  #complete(sentence.id, word) %>% 
  spread(key = word, value = count, fill = 0, convert = T) # use fill to replace NA's with 0

sona_matrix[1:10,1:20]
```

Clean matrix by moving the sentence.id into the row name, and adding the presidents name as a predictor variable.

```{r}
# create predicter vector of president names
predicter_presidents_name = sona_sentences[sona_sentences$sentence.id %in% sona_matrix$sentence.id,]$president

sona_rownames = sona_matrix$sentence.id

# remove sentence.id
sona_matrix = sona_matrix[,-1]

sona_colnames = colnames(sona_matrix)

# turn into a matrix
sona_matrix = as.matrix(sona_matrix)

# make sentence.id the row name
rownames(sona_matrix) = sona_rownames
colnames(sona_matrix) = sona_colnames

sona_matrix[1:10,1:20]
```

Confirm the size of our matrix

```{r}
dim(sona_matrix)
```

Confirm the number of presidents

```{r}
sona_words %>%
  group_by(president) %>%
  summarise(word.count = sum(count)) %>%
  mutate(word.percentage = round(word.count / sum(sona_words$count) * 100, 2)) %>%
  select(president, word.count, word.percentage) %>%
  arrange(word.count)
```

```{r}
#sona_sentences %>%
#  group_by(year, president) %>%
#  summarise(sentence.count = n()) %>%
#  arrange(year)

sona_sentences %>%
  group_by(president) %>%
  summarise(sentence.count = n()) %>%
  mutate(sentence.percentage = round(sentence.count / nrow(sona_sentences) * 100, 2)) %>%
  mutate(max.count = max(sentence.count)) %>%
  mutate(adjust.class.weight = round(max.count / sentence.count, 1)) %>%
  select(president, sentence.count, sentence.percentage, adjust.class.weight) %>%
  arrange(sentence.count)
```

```{r}
# save R objects
# test
save(sona_files, sona_data, sona_sentences, sona_words, sona_matrix, predicter_presidents_name, file = "Main_SONA_objects.R")
```

