---
title: "STA5076Z Assignment 2: SONA Predict the President"
author: "Marike du Plessis (DPLMAR037), Waldo Leonhardt (LNHWAL001) and Francois Evert (EVRFRA002)"
date: "13 September 2018"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction 

This is the second assignment for Data Science for Industry where the goal is to use as input the State of the Nation Address (SONA) speeches from 1994 to 2018 and achieve the following:

1.	Build a neural network classifier which can predict who was the president from any given sentence extracted from a speech.
2.	Assess the out-of-sample performance of the classifier.
3.	Conduct a descriptive analysis of the text in the speeches.


## Load data

Start by extracting data out of the SONA files.
Build a data frame containing the filename, the speech text, president and year of speech.

```{r}
library(tidyverse)

# extract list of SONA files
# NOTE: that file name "2009_pre_elections_ Motlanthe.txt" has been corrected to remove the space before Motlanthe as this will cause issues later
#       when the president name needs to be assosiated with matrix entries
sona_files = list.files("data/")

# create empty data frame for SONA data
sona_data = data.frame(filename = as.character(), speech = as.character(), president = as.character())

# step through files and build sona_data
for(i in sona_files){
  this_file = paste0("data/", i)
  
  # extract president
  this_file_name = str_replace(i, ".txt", "") # remove .txt from file name
  
  this_president = str_sub(this_file_name, start = last(unlist(str_locate_all(this_file_name, "_")))+1, end = nchar(this_file_name))
  
  # extract speech text as single character string (can also read.table but the "seperator" causes problems)
  this_speech = readChar(this_file,
                         nchars = file.info(this_file)$size)
  
  # make data frame with metadata (filename contains year and pres) and speech
  this_sona = data.frame(filename = i, speech = this_speech, president = this_president, stringsAsFactors = FALSE)
  
  # add row to sona_data
  sona_data = rbind(sona_data, this_sona)
}

# add year to SONA data
sona_data$year = str_extract(sona_data$filename, "[0-9]{4}") # year is the first 4 numbers

head(sona_data)
```
  
The below histogram shows the amount of speeches we have available from our data per president. It is clear that we have a lot of speeches for Mandela, Mbeki and Zuma, but for Ramaphosa, de Klerk and Motlanthe only a single speech each is available.    
  
```{r}
library(ggplot2)
ggplot(data=sona_data, aes(sona_data$president)) + 
  geom_bar(fill="darkblue")+
  labs(title="Counts of speeches per president", x="President Name", y="Count") +    
  scale_y_continuous(breaks = c(2,4,6,8,10))
```

We can now extract the sentences within each speech. To do this we will use the unnest_tokens function in package tidytext.

```{r}
library(tidytext)

# we want to predict sentences, so we need to first split into sentences
# use the speech attribute and replace with a sentence attribute, tokenized by sentences
sona_sentences = sona_data %>% unnest_tokens(sentence, speech, token = "sentences")

# convert to lower case
sona_sentences$sentence = str_to_lower(sona_sentences$sentence)

head(sona_sentences)
```

We would also like to extract the words within sentences. We can also do this using the unnest_tokens function.

```{r}
# add an ID variable for sentences
sona_sentences$sentence.id = rownames(sona_sentences) # use row number and assign to new ID variable

# use the sentence attribute and replace with a word attribute, tokenized by words
sona_words = sona_sentences %>% unnest_tokens(word, sentence, token = "words")

head(sona_words)
```
  
We further explore our data by taking an overall look at frequent words (excluding stopwords) used and which president used these words as the plot below shows.  
  
```{r}
#count how many times each word was used overall (all speeches) per president

per_pres = sona_words %>%
  filter(!word %in% stop_words$word,str_detect(word, "[a-z]")) %>%
  group_by(president) %>%
  count(word, sort = TRUE) %>%
   filter(rank(desc(n)) <= 20)


ggplot(per_pres[1:20,], aes(y=n, x=word))+
  geom_col(aes(fill=president), position="dodge") + theme_minimal() + coord_flip() +
  labs(title="Top 20 repeated words overall", x="Word", y="Count") 

```
  
List the top 30 most frequent words used:
  
```{r}
# view word frequencies
sona_words %>%
  count(word, sort = TRUE) %>%
  filter(rank(desc(n)) <= 30) # list top 30 ranked words
```

As expected, the most frequent words are stop-words. Let's remove those first.

```{r}
# remove stop-words
sona_words = sona_words %>%
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %>% # remove stop words
  #filter(!str_detect(word, "[[:punct:]]")) %>% # remove punctuation NOTE: this will remove all words containing any punctuation
  select(-filename)

head(sona_words, 30)
```

Let's look again at the words occuring the most when ignoring stop-words:

```{r}
# view word frequencies
sona_words %>%
  count(word, sort = TRUE) %>%
  filter(rank(desc(n)) <= 30) # list top 30 ranked words
```

We can add a count of words per sentence and list the sentences where the same word occurs the most frequent. Interestingly, the word that occurs most frequently in a sentence is actually the Afrikaans word "die" when Mbeki was quoting a poem. This should not be removed from our set, as Afrikaans stop words in an Afrikaans sentence (when predicting on that) will also be considered, hence we keep this in.

```{r}
sona_words = sona_words %>%
  group_by(president, year, sentence.id, word) %>%
  summarise(count = n())

# list sentences with the most reoccuring words
sona_words %>%
  arrange(desc(count)) %>%
  head(30)
```

To use the data in a neural network we would need to reshape the data in matrix form.

```{r}
# build matrix of words per sentence.id
sona_matrix = sona_words %>%
  ungroup() %>% # remove grouping, otherwise complete and spread will use variables 'president' and 'year'
  select(sentence.id, word, count) %>%
  #complete(sentence.id, word) %>% 
  spread(key = word, value = count, fill = 0, convert = T) # use fill to replace NA's with 0

sona_matrix[1:10,1:20]
```

Clean tibble by moving the sentence.id into the row name, and adding the presidents name as a predictor variable.

```{r}
# create predicter vector of president names
predicter_presidents_name = sona_sentences[sona_sentences$sentence.id %in% sona_matrix$sentence.id,]$president

sona_rownames = sona_matrix$sentence.id

# remove sentence.id
sona_matrix = sona_matrix[,-1]

sona_colnames = colnames(sona_matrix)
sona_tibble = sona_matrix
# turn into a matrix
sona_matrix = as.matrix(sona_matrix)

# make sentence.id the row name
rownames(sona_matrix) = sona_rownames
colnames(sona_matrix) = sona_colnames

#sona_matrix[1:10,1:20]
```

Confirm the size of our data:

```{r}
dim(sona_matrix)
```

We can also list the presidents by the number of sentences we have for them and calculate an adjustment weight which can be applied if we would like to create an equal balance of sentences among them.

```{r}
#sona_sentences %>%
#  group_by(year, president) %>%
#  summarise(sentence.count = n()) %>%
#  arrange(year)

sona_sentences %>%
  group_by(president) %>%
  summarise(sentence.count = n()) %>%
  mutate(sentence.percentage = round(sentence.count / nrow(sona_sentences) * 100, 2)) %>%
  mutate(max.count = max(sentence.count)) %>%
  mutate(adjust.class.weight = round(max.count / sentence.count, 1)) %>%
  select(president, sentence.count, sentence.percentage, adjust.class.weight) %>%
  arrange(sentence.count)
```
  
## Topic modelling

We want to investigate what topics the SONA speeches touched on. As we have no idea how many topics there might be, we take a value of 6 (1 per president) as this will make the most sense intuitively. Our aim is to name and interpret the topics.   
  
We need a tibble, per president, that shows the word and the count (how many times this word occurs per president.) This data is shown below. Note that we also stem the words to avoid having cases like 'child' different to 'children' which came up as main topic differentiators with unstemmed words. The summary below shows how we group the word counts together per president.    
  
```{r}
library(SnowballC)

sona_tdf = sona_words %>%
  mutate(word=wordStem(word)) %>%
  group_by(president,word) %>%
  count() %>%  
  ungroup() 
head(sona_tdf)
```
  
Then we cast this into a DocumentTermMatrix.  
  
```{r, warning=FALSE, message=FALSE}
library(topicmodels)
library(tidyverse)
library(tidytext)
dtm_sona =  cast_dtm(sona_tdf, president, word, n)

```
  
Latent Dirichlet Allocation will be used for topic modelling. It basically labels a topic as a mixture of words and a document as a mixture of topics. 
  
```{r}
sona_lda <- LDA(dtm_sona, k = 6, control=list(seed=123))
#sona_lda
#str(sona_lda)
```
    
### Investigating what our topics are about  
  
We need to focus on 2 parameters to establish what our topics are about:
  
* `beta`: these parameters control the probability of a given topic k generating a particular word i.
* `gamma`: this gives the topic "mixtures" for each document.
  
As an example, in the below we can see the word 'absolut' has probabilities of 'beta' to be in each topic as displayed below.  
  
```{r}
library(tidytext)
sona_topics <- tidy(sona_lda, matrix = "beta") 
 head(sona_topics)
```

Now we extract the top 5 words per topic and plot it to display visually:
  
```{r}
top_terms <- sona_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) 

top_terms %>%
  #mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  labs(title="Top 5 stem words per topic", x="", y="Beta") +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

If we consider the most used words, we can roughly interpret our topics as below: 

* Topic 1 has $south$, $govern$, $people$, $country$ and $improvement$ stem words listed. The unique words when comparing to other topics are $improvement$, so we can summarise this topic as improving South Africa.  
  
* Topic 2 has $govern$, $nation$, $people$, $country$ and $public$ stem words listed in the top 5. Unique words here are $public$ hence we can conclude that this topic is about governing the nation, with a specific emphasis on public relations.

* Topic 3 has $govern$, $south$, $service$, $continous$ and $african$ stem words with unique words as $service$ and $continous$. Hence we can summarise this topic as continous service for south africa.  
  
* Topic 4 has $south$, $constitution$, $people$, $african$ and $nation$ as stem words in the top 5. The focus here is strongly on the constitution (unique word compared to others) as topic.  
  
* Topic 5 has $people$, $govern$, $country$, $programme$ and $nation$ in its top 5 stem words. This topic is about programmes for the country.  
  
* Topic 6 listed $govern$, $south$, $nation$, $develop$ and $country$ in its top 5 stem words. A unique words seen here is $develop$, hence this topic is about development of South Africa. 

We also focus on the log ratio differences between words to see if we can better establish the differences between topics. We need to filter out common words (words with beta >= 0.001) in at least one topic. Using $log_2(\beta_2/\beta_1)$ we can model the differences between 2 topics. I.e. if $\beta_2$ is twice as large as $\beta_2$, the log ratio will be 1, whereas if $\beta_1$ is twice as large as $\beta_2$, the log ratio will give a value of -1. Filtering on greater log difference values (e.g. $>=0.0001$) in one topic gives an indication of the contrast.  

```{r, warning=FALSE}
beta_spread <- sona_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001 | topic3 > .001 |topic4 > .001 |topic5 > .001 |topic6 > .001 ) %>%
  mutate(log_ratio_1 = log2(topic2 / topic1), log_ratio_3 = log2(topic3 / topic1), log_ratio_4 = log2(topic4 / topic1), log_ratio_5 = log2(topic5 / topic1), log_ratio_6 = log2(topic6 / topic1)) 

beta_spread %>%
  group_by(direction = log_ratio_1 > 0) %>%
  top_n(5, abs(log_ratio_1)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio_1)) %>%
  ggplot(aes(term, log_ratio_1)) +
  labs(title="Log2 ratio of beta in topic 2 / topic 1", x="Words", y="Ratio") +
  geom_col() +
  #scale_y_continuous(limits=c(-300,10000000000000)) +
  coord_flip()

beta_spread %>%
  group_by(direction = log_ratio_3 > 0) %>%
  top_n(5, abs(log_ratio_3)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio_3)) %>%
  ggplot(aes(term, log_ratio_3)) +
  labs(title="Log2 ratio of beta in topic 3 / topic 1", x="Words", y="Ratio") +
  geom_col() +
  coord_flip()

beta_spread %>%
  group_by(direction = log_ratio_4 > 0) %>%
  top_n(5, abs(log_ratio_4)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio_4)) %>%
  ggplot(aes(term, log_ratio_4)) +
  labs(title="Log2 ratio of beta in topic 4 / topic 1", x="Words", y="Ratio") +
  geom_col() +
  coord_flip()

beta_spread %>%
  group_by(direction = log_ratio_5 > 0) %>%
  top_n(5, abs(log_ratio_5)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio_5)) %>%
  ggplot(aes(term, log_ratio_5)) +
  labs(title="Log2 ratio of beta in topic 5 / topic 1", x="Words", y="Ratio") +
  geom_col() +
  coord_flip()

beta_spread %>%
  group_by(direction = log_ratio_6 > 0) %>%
  top_n(5, abs(log_ratio_6)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio_6)) %>%
  ggplot(aes(term, log_ratio_6)) +
  labs(title="Log2 ratio of beta in topic 6 / topic 1", x="Words", y="Ratio") +
  geom_col() +
  coord_flip()
```

Interpreting the log ratio differences, we see the below when we compare each topic to topic 1 (to shed more light on the meaning and inclusion/exclusion of the topics):  

* Topic 1 (Improvement) differs from topic 2 (Public) the most by stem words like $offens$, $discharge$, $success$, $ministries$ and $farmer$ seen in topic 2 and $stronger$, $april$ seen mostly in topic 1. Note the word "van" is an Afrikaans stop word, hence significant only because Afrikaans was used in topic 1. 
  
* Topic 3 (Continous service for SA) differs from topic 1 again by not frequently using words like $farmer$, $offens$, $discharge$ and $success$ as seen earlier. Also we see stem words such as $underdevelop$ used in topic 3 more frequently and topic 1 has $even$ used, that we do not see as frequently in topic 3. This makes sense considering topic 3 wants continous service for SA, hence will discuss underdevelopment whereas topic 1 focuses more on improvement and $even$ words. 

* Topic 4 (Constitution) also inclused words like $farmer$ as seen above that topic 1 did not use frequently. But we see words like $even$, $compatriot$, $limpopo$, $pleas$ used more in this topic than topic 1. This meanse the word $even$ is seen in this case much more in topic 4 than 1, whereas we saw the word $even$ more in topic 1 than 3. Our conclusion here is that the constitution-topic focuses more strongly on $even$.   

* Topic 5 (Programmes) differs also by using stem words like $discharg$, $underdevelop$, $offens$, $limpopo$ and $compatriot$ but it contains far fewer occurences of topic 1 words like $virtual$ and $even$.

* Topic 6 (Development) contains stem words like $even$, $compatriot$, $limpopo$, $underdevelop$, $farmer$ but fewer $virtual$ and $instance$ and $chamber$ words than topic 1. 
    
## Document topic probabilities  
Gamma is the mixture of topics per president, so the below table shows which topic is used by which president. 
  
```{r}

sona_gamma = tidy(sona_lda, matrix="gamma")

sona_gamma %>%
  mutate(document = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  labs(title="Boxplots of topic per president") +
  geom_boxplot() +
  facet_wrap(~ document)

```
  
From above, we interpret the topics per president as below (note the topic modelling distinguished quite well between presidents) :  

De Klerk's speech consists mainly of topic 4 (Constitution.) This makes sense considering this is the last speech before apartheid ended and many things changed constitutionally thereafter (as well as enabling all people to vote.) 

Mandela's speech used both topic 3 (less) and topic 2 (higher percentage) with his speeches. Topic 3 covers continous service and topic 2 the public relations. This was a time for transition for the country and uniting the nation (strong public relations) would have been an important topic to cover considering the changes made in 1994. It is interesting to note that both Mandela and his successor, Mbeki, had SONAs which contained topic 3 (continous service) as this was an important topic for a young democracy.    
  
Topic 5 (Programmes) makes up a larger portion of Mbeki's speech. Still continuing service, but now after uniting the nation with good public relations (Mandela's topics) the focus shifts with Mbeki to programmes to develop the country. This was a time of economic growth for South Africa, so his focus on programmes makes sense for more development. 

Motlanthe took over from Mbeki and we see his speech mainly consisted of topic 1 (improving SA.) This topic is unique to Motlanthe and considering he was only president until they could appoint Zuma, his main focus was on improvement.
  
Zuma's speeches mainly covered topic 6 (Development) but also contained elements similar to de Klerk's SONA topic 4 (Constitution). Once again it is interesting to note that he 'handed over' to Ramaphosa and we can see a small percentage of Zuma's topics as topic 4 (constituion) but this topic largely identifies Ramaphosa's SONA. 
  
## Wordclouds for our presidents  
We can use our stem-words to show the most used words per president. For the example below, we show the most used (stem) words of Mandela in a word cloud of 25 words as shown below. As expected, the words $govern$, $nation$ and $people$ stand out.  
  
```{r}
library("wordcloud")
sona_tdf %>%
  filter(president=='Mandela') %>%
  with(wordcloud(word, n, max.words = 25))
```
  
It would be interesting to do sentiment analysis per president (or per year) to see how the context of SONAs changed - was it more negative or positive?  

## Sentiment analysis  

We use the bing lexicon (words are labelled as positive or negative) to get an idea of each president's speech, but also see how sentiment changed over time. Note we will not do this on the stem words, but on the actual words, as the sentiment is dependant on more than the stem words itself, so we need to keep the whole word in.

Below shows the sentiments for specific words.

```{r}
get_sentiments("bing") %>% head(10)
```

We use our tidyframe that is tokenised by words (not stemmed) and add the sentiment analysis on this. The result is as below. Note we change all the NA words to neutral for clarity.
 
```{r}
sona_words_sent = sona_words %>% 
  left_join(get_sentiments("bing")) %>% # add sentiments (pos or neg)
  select(word,sentiment,everything()) %>%
  mutate_each(funs(replace(., is.na(sentiment), "neutral")))

head(sona_words_sent)
```

Now we can look again at the most used positive words per president, which president used which positive words (and how many times overall)?  

```{r}
sona_words_sent %>%
  ungroup %>%
  select('word', 'sentiment', 'president') %>%
  filter(sentiment == "positive") %>%
  group_by(president) %>%
  count(word) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 4) %>%
  ggplot(aes(x=reorder(word, n),y=n, fill=president)) + geom_col() + coord_flip() + 
  labs(title="Most occuring positive words per president used", x="Word", y="Count")
```
  
We can do the same for the negative words, which president used the mostly used negative words and how many times during his reign as president?  
  
```{r}
sona_words_sent %>%
  ungroup %>%
  select('word', 'sentiment', 'president') %>%
  filter(sentiment == "negative") %>%
  group_by(president) %>%
  count(word) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 4) %>%
  ggplot(aes(x=reorder(word, n),y=n, fill=president)) + geom_col() + coord_flip() +
  labs(title="Most occuring negative words per president used", x="Word", y="Count")
```

It is interesting to note that crime has been a hot topic with Zuma, Mandela and Mbeki mentioning it often during their presidential SONA speeches. Poverty is a recurring negative theme with the last 4 presidents mentioning that frequently in their speeches. We also see corruption frequently mentioned by Zuma and Ramaphosa, and that is all I am going to say about that...

In summary, if we combine all speeches and want to know which were the most used positive and negative words, a wordcloud can also represent it as shown below, the negative words are shown in red at the top, with the positive words in aqua-colour.   

```{r, warning=FALSE}
library(reshape2)

sona_words_sent %>%
  ungroup() %>%
  filter(sentiment != 'neutral') %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 25, title.size = 1)
```
  
#Changing sentiments over time (per year per speech)  
  
We would like to see how the sentiments changed per year - did it grow generally more negative, or more positive? Instead of investigating per president, we would like to trend the positive/negative ratio per year. In other words, what percentage of total words used were positive, what percentage was negative, and how did this change over time?  
  
```{r, warning=FALSE, message=FALSE}
sona_sent_plot = 
sona_words_sent %>%
  ungroup %>%
  select('sentiment', 'year', 'president') %>% 
  count(sentiment, year, president) %>%
  spread(sentiment, n, fill=0) %>%
  mutate(sentiment_diff = positive-negative, 
         sentiment_norm = (positive - negative) / (positive+negative+neutral) * 100)

# plot of sentiment over time & automatically choose a method to model the change
ggplot(sona_sent_plot, aes(x = as.numeric(year), y = sentiment_diff)) + 
  geom_point(aes(color = president))+ # add points to our plot, color-coded by president
  geom_smooth(method = "auto") + # pick a method & fit a model
  labs(title="Sentiment difference over time per president", x="Year", y="Delta (positive-negative) words") + 
   scale_x_continuous(breaks = c( seq(1994, 2018, 2)))

# plot of sentiment over time & automatically choose a method to model the change
ggplot(sona_sent_plot, aes(x = as.numeric(year), y = sentiment_norm)) + 
  geom_point(aes(color = president))+ # add points to our plot, color-coded by president
  geom_smooth(method = "auto") + # pick a method & fit a model
  labs(title="Sentiment ratio over time per president", x="Year", y="Ratio of ((positive-negative)/total) words") +
   scale_x_continuous(breaks = c( seq(1994, 2018, 2)))  
  
```
  
From the above we can see that de Klerk's speech was quite negative, this was a very uncertain time for the country and it comes across in his speech's negativity. Mandela focused a lot on building community (public relations and contious services topics) which inherently is more positive words. Generally speaking, the trend continued to increase positively when Mbeki took over from Mandela in 2000. We can see the ratio flattening around 2005 and the sentiment ratio of Mbeki was almost as low in 2008 as when he started his presidency in 2000. The year of 2008 is also the year that the world-wide recession hit and the outlook was indeed bleak that year. 

Motlanthe took over for a short while until Zuma could be president in 2009. Zuma was very positive in 2010 and 2011, but thereafter (with the exception of 2015) he did not reach such positive sentiment ratios again. An interesting trend we see is that each president, when it is their last SONA, ended with a SONA that has a sentiment ratio below (more negative than) the smooth average curve drawn above. Zuma also ended his SONA with a negative ratio and Ramaphosa was very positive again when he became president in 2018. 


## NRC lexicon

* $Parliament$ is associated with trust
* $Republic$ is marked as negative
* $Government$ is marked as negative

The above is a couple of examples that shows that the NRC lexicon will not make sense for a SONA where these words will be used regularly.


## AFINN lexicon

AFINN scores each word with a negative to positive score (-5 up to +5). If we compare the word sentiment (no negation) using AFINN with the word sentiment (no negation) with BING lexicon, the only difference we see is obviously the scores that is more extreme with AFINN (as expected, with BING we weighted all words from -1 to +1) leading to a graph that is steeper overall on average. The trends are the same, with the only notable difference that Motlanthe is rated by AFINN as slightly more positive than the average smoothed line, wheras the BING lexicon rated Motlanthe as slightly more negative than this smoothed line.

```{r, message=FALSE}
#get_sentiments("afinn") %>% head(10)

#number of words of each sentiment level
new_one =
sona_words %>%
  ungroup() %>%
  inner_join(get_sentiments("afinn")) %>% # add sentiments (pos or neg)
  select(president,year, count, score) %>%
  mutate(total_score = count*score) %>%
  group_by(president, year) %>%
  summarise(sentiment = sum(total_score),
            sentiment_norm = sum(total_score) / nrow(sona_words) * 100)

# plot of sentiment over time & automatically choose a method to model the change
ggplot(new_one, aes(x = as.numeric(year), y = sentiment_norm)) +
  geom_point(aes(color = president))+ # add points to our plot, color-coded by president
  geom_smooth(method = "auto") + # pick a method & fit a model
  labs(title="AFINN normalised sentiment ratio over time per president", x="Year", y="Ratio of sum(score)/total words") +
  scale_x_continuous(breaks = c( seq(1994, 2018, 2)))

```


## Bigram Sentiment with Negation
  
One-step bigrams are created and if a negation word is used, the initial sentiment is reversed.    

```{r, message=FALSE}

bigrams_separated  = sona_sentences %>%
  #mutate(sentence = str_replace_all(sentence, replace_reg, "")) %>%
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_separated <- bigrams_separated %>% 
    # add sentiment for word 1
    left_join(get_sentiments("bing"), by = c(word1 = "word")) %>%
    rename(sentiment1 = sentiment) %>%
    mutate(sentiment1 = ifelse(is.na(sentiment1), "neutral", sentiment1)) %>%
    
    # add sentiment for word 1
    left_join(get_sentiments("bing"), by = c(word2 = "word")) %>%
    rename(sentiment2 = sentiment) %>%
    mutate(sentiment2 = ifelse(is.na(sentiment2), "neutral", sentiment2)) %>%
    select(president,word1,word2,sentiment1,sentiment2,everything())
# head(bigrams_separated)

negation_words <- c("not", "no", "never", "without")

# show a few
filter(bigrams_separated, word1 %in% negation_words, sentiment2  != "neutral") %>% 
    head(10) %>% select(president, word1, word2, sentiment1, sentiment2) # for display purposes

bigrams_separated <- bigrams_separated %>%   
  
    # create a variable that is the opposite of sentiment2
    mutate(opp_sentiment2 = recode(sentiment2, "positive" = "negative",
                                 "negative" = "positive",
                                 "neutral" = "neutral")) %>%
    
    # reverse sentiment2 if word1 is a negation word
    mutate(sentiment2 = ifelse(word1 %in% negation_words, opp_sentiment2, sentiment2)) %>%
    
    # remove the opposite sentiment variable, which we don't need any more
    select(-opp_sentiment2)

bigrams_separated <- bigrams_separated %>%
  mutate(net_sentiment = (sentiment1 == "positive") + (sentiment2 == "positive") - 
              (sentiment1 == "negative") - (sentiment2 == "negative")) %>%
  unite(bigram, word1, word2, sep = " ", remove = FALSE)

bigrams_separated %>%
  filter(net_sentiment > 0) %>% # get positive bigrams
  count(bigram, sort = TRUE) %>%
  filter(rank(desc(n)) < 20) %>%
  ggplot(aes(reorder(bigram,n),n)) + geom_col() + coord_flip() + xlab("") + labs(title = "Count of Positive Bi-grams", x = "Count")

bigrams_separated %>%
  filter(net_sentiment < 0) %>% # get negative bigrams
  count(bigram, sort = TRUE) %>%
  filter(rank(desc(n)) < 20) %>%
  ggplot(aes(reorder(bigram,n),n)) + geom_col() + coord_flip() + xlab("") + labs(title = "Count of Negative Bi-grams", x = "Count")

bigrams_separated %>%
    filter(net_sentiment > 0) %>% # get Positive bigrams
    filter(word1 %in% negation_words) %>% # get bigrams where first word is negation
    count(bigram, sort = TRUE) %>%
    filter(rank(desc(n)) < 20) %>%
    ggplot(aes(reorder(bigram,n),n)) + geom_col() + coord_flip() + xlab("") + labs(title = "Count of Postive Bi-grams where 1st word is negation.", x = "Count")

bigrams_separated %>%
    filter(net_sentiment < 0) %>% # get negative bigrams
    filter(word1 %in% negation_words) %>% # get bigrams where first word is negation
    count(bigram, sort = TRUE) %>%
    filter(rank(desc(n)) < 20) %>%
    ggplot(aes(reorder(bigram,n),n)) + geom_col() + coord_flip() + xlab("") + labs(title = "Count of Negative Bi-grams where 1st word is negation.", x = "Count")


bigrams_separated_ratio = bigrams_separated %>% 
  ungroup() %>%
  select(year, president, net_sentiment) %>%  
  group_by(president,year) %>%
  summarise(sentiment_norm = sum(net_sentiment)/n())

# bigrams_separated_ratio

# plot of sentiment over time & automatically choose a method to model the change
ggplot(bigrams_separated_ratio, aes(x = as.numeric(year), y = sentiment_norm)) + 
  geom_point(aes(color = president))+ # add points to our plot, color-coded by president
  geom_smooth(method = "auto") + # pick a method & fit a model
  labs(title="Sentiment ratio over time per president with Negation", x="Year", y="Ratio of (sum(Bigram sentiment)/total bigrams") +
   scale_x_continuous(breaks = c( seq(1994, 2018, 2)))  

```

It is interesting to note that, when employng Negation, the initial negative sentiment of De Klerk, based on words without negation, has changed to be positive. Additionally, the elevated positive sentiment between 2002 to 2007, has been reduced. Unlike the non-negated word sentiment analysis, negation also highlights a steeper negative trend from 2016 onwards. The  trend that each president, when it is their last SONA, ended with a SONA that has a sentiment ratio below (more negative than) the smooth average curve drawn above, is however maintained.

## Term Frequency - Inverse Document Frequency (TF-IDF)
TF-IDF is a method of assigning weight to more important words in documents. It comprises of 2 separate indicators, which combines to produce the final weight.

1. Term frequency (TF) : This is a calculation of how frequent a term is found in a document, and is given by the number of occurrences of the term in a document, divided by the number of terms in that document. \n

$TF_{ij} = \frac{Occurrences_{ij}}{Terms_j}}$ for all terms i in document j. \n

2. Inverse document Frequency : This is the natural log (Ln) of the total number of documents, divided by the number of documents in which the term appears. \n
$IDF_i = Ln({\frac{Documents_{Tot}}{DocumentsWithTerm_i})}$ for all terms i

The simple multiplication of these 2 values produce the tf-idf for each term in each document. Applying the *bind_tf_idf* function in R to the SONA word tibble, adds the TF, IDF and TF-IDF features to the tibble. The 6 figures below show the top 10 most important words of each president.

It would seem that over the years, the perceived political weight of the words decreased over time. As such, presidents De Klerk and Mandela used words with big political implication such as "autonomy", "volkstaat", "constitution", "democracy" and "truth, while the later presidents had less of such "heavy-weight" terms in their top 10 most important words.

```{r fig.width=8, fig.height=10}

# head(sona_data)

# president is also a word in the text, so rename to be safe
sona_bow = sona_data %>% rename(president_sona = president) %>%
  unnest_tokens(word_sona, speech, token = "words") %>%
  filter(!word_sona %in% stop_words$word,str_detect(word_sona, "[a-z]")) %>%
  group_by(president_sona, word_sona) %>%
  summarise(count_sona = n()) %>%
  ungroup()

 # head(sona_bow)

# Using built in function (https://www.tidytextmining.com/tfidf.html)
sona_bow_tfidf = sona_bow %>% 
  bind_tf_idf(word_sona, president_sona, count_sona)

# head(sona_bow_tfidf)
# sona_bow_tfidf %>% group_by(president_sona) %>% summarise(n())


# Plotting the top 10 most important words per president 
sona_bow_tfidf %>%
  group_by(president_sona) %>% 
  arrange(desc(tf_idf)) %>%
  # mutate(word_sona = factor(word_sona, levels = rev(unique(word_sona)))) %>% 
  top_n(10) %>% 
  ungroup %>%
  ggplot(aes(x = reorder(word_sona,tf_idf,min), y = tf_idf, fill = president_sona)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~president_sona, ncol = 2, scales = "free") +
  coord_flip()

```


## Bag of Words
Bag-of-Words is structured representation of the words used in sets of documents (*corpus*). In the "Bag" the order, punctuation and subtle implication ("between the lines" meaning) of each word is disregarded, and merely the occurrence count per document is maintained in the representation. Such Bag-of-words information sets are often used as input to descriptive and predictive modelling. 

To illustrate this, a simple tree model is fitted on the SONA data, using the president's name as the "document" identifier. This yielded a training accuracy of 43% and at test accuracy of 42%. (Better models can be built using the bag-of-words dataset, as is shown later in this document.) 


```{r}
library(rpart)
set.seed(123)

sona_bow = sona_words %>% 
  rename(president_sona = president, word_sona = word, count_sona = count, year_sona = year) %>%
  spread(word_sona,count_sona,fill = 0)

train_ids <- sona_bow %>% 
  group_by(president_sona) %>% 
  sample_frac(0.6) %>% 
  ungroup() %>%
  select(sentence.id)

train_bow <- sona_bow %>% 
  ungroup() %>%
  right_join(train_ids, by = "sentence.id") %>%
  select(-year_sona, -sentence.id)

test_bow <- sona_bow %>% 
  ungroup() %>%
  anti_join(train_ids, by = "sentence.id") %>%
  select(-year_sona, -sentence.id)

# head(sona_bow)
# head(train_bow)
# head(test_bow)

bow_fit <- rpart(factor(president_sona) ~ ., train_bow)

options(repr.plot.width=8, repr.plot.height=10)
plot(bow_fit, main="Full Classification Tree")
text(bow_fit, use.n=TRUE, all=TRUE, cex=.7)

```
  
Below output shows the training set classification and accuracy.  
  
```{r}
fittedtrain <- predict(bow_fit,type="class")
predtrain <- table(train_bow$president_sona,fittedtrain)
predtrain
sum(diag(predtrain))/sum(predtrain) # training accuracy
```
  
It is very similar to the test set classification accuracy:  
  
```{r}
fittedtest <- predict(bow_fit,newdata=test_bow,type="class")
predtest <- table(test_bow$president_sona,fittedtest)
predtest
sum(diag(predtest))/sum(predtest) # test accuracy
```

The object can be saved.  
```{r}
save(sona_bow, train_ids, train_bow, test_bow, bow_fit, fittedtrain, predtrain, fittedtest, predtest, file = "models/BOW.RData")
```


## Bag of Words with TF-IDF
An alternative implementation for Bag-of-Words is to replace the word occurrence count per document with the term frequency - inverse document frequency (tf-idf) of each word. This could enhance the value of the bag-of-words by giving weight to more important words used in the documents. 

Again, to illustrate the use, a simple tree model is fitted on the SONA data, using tf-idf in this case, and the president's name as the "document" identifier. In this case marginally better training accuracy of 44% and a worse test accuracy of only 10% were achieved. (Note that the number of words were limited to the top 2000 based on TF-IDF.)


```{r}
library(rpart)
set.seed(123)

head(sona_words)
sona_words_tfidf = sona_words %>% 
  rename(president_sona = president, word_sona = word, count_sona = count, year_sona = year) %>%
  bind_tf_idf(word_sona, sentence.id, count_sona)
head(sona_words_tfidf)

sona_bow_tfidf = sona_words_tfidf %>% 
  group_by(president_sona) %>% 
  top_n(2000,tf_idf) %>% 
  ungroup() %>% 
  spread(word_sona,tf_idf,fill = 0)
head(sona_bow_tfidf)

train_ids_tfidf <- sona_bow_tfidf %>% 
  group_by(president_sona) %>% 
  sample_frac(0.6) %>% 
  ungroup() %>%
  select(sentence.id)

train_bow_tfidf <- sona_bow_tfidf %>% 
  ungroup() %>%
  right_join(train_ids_tfidf, by = "sentence.id") %>%
  select(-year_sona, -sentence.id)

test_bow_tfidf <- sona_bow_tfidf %>% 
  ungroup() %>%
  anti_join(train_ids_tfidf, by = "sentence.id") %>%
  select(-year_sona, -sentence.id)

head(sona_bow_tfidf)
head(train_bow_tfidf)
head(test_bow_tfidf)

fit_tfidf <- rpart(factor(president_sona) ~ ., train_bow_tfidf)

options(repr.plot.width=8, repr.plot.height=10)
plot(fit_tfidf, main="Full Classification Tree with TF-IDF")
text(fit_tfidf, use.n=TRUE, all=TRUE, cex=.7)

```


The training set has accuracy as shown below:  
  
```{r}
fittedtrain_tfidf <- predict(fit_tfidf,type="class")
predtrain_tfidf <- table(train_bow_tfidf$president_sona,fittedtrain_tfidf)
predtrain_tfidf
sum(diag(predtrain_tfidf))/sum(predtrain_tfidf) # training accuracy
```

The test set has accuracy as below:
```{r}
fittedtest_tfidf <- predict(fit_tfidf,newdata=test_bow_tfidf,type="class")
predtest_tfidf <- table(test_bow_tfidf$president_sona,fittedtest_tfidf)
predtest_tfidf
sum(diag(predtest_tfidf))/sum(predtest_tfidf) # test accuracy
```

The objects are saved as well.  

```{r}
save(sona_words_tfidf, sona_bow_tfidf, train_ids_tfidf, train_bow_tfidf, test_bow_tfidf, fit_tfidf, fittedtrain_tfidf, predtrain_tfidf, fittedtest_tfidf, predtest_tfidf, file = "models/TFIDF.RData")
```

## Feed-forward neural network - with tfidf  
  
A feed-forward neural network, using tfidf, can be used to predict which president used a particular sentence in his SONA. The data is split up into a test and train set (70% to 30%).  
  

```{r}
library(keras)
set.seed(123)

# sona_bow_tfidf <- sona_words_tfidf %>% 
#   ungroup() %>% 
#   select(president_sona, sentence.id, word_sona, tf_idf) %>%  # note the change, using tf-idf
#   group_by(president_sona) %>% 
#   top_n(2000) %>% 
#   ungroup() %>% 
#   spread(key = word_sona, value = tf_idf, fill = 0) 

pres_lookup = as.tibble(as.matrix(cbind(pres_nr = as.numeric(c(0,1,2,3,4,5)) ,
              president_sona = c('deKlerk','Mandela','Mbeki','Motlanthe','Zuma','Ramaphosa'))))

sona_bow_tfidf = sona_bow_tfidf %>%
  left_join(pres_lookup, by = "president_sona") %>%
  select(president_sona, pres_nr, sentence.id, everything())

unique(sona_bow_tfidf$pres_nr)

train_ids <- sona_bow_tfidf %>% 
  group_by(president_sona) %>% 
  sample_frac(0.7) %>% 
  ungroup() %>%
  select(sentence.id)

train_bow_tfidf <- sona_bow_tfidf %>%
  ungroup() %>%
  right_join(train_ids, by = "sentence.id") %>%
  select(-sentence.id)

unique(train_bow_tfidf$pres_nr)

test_bow_tfidf <- sona_bow_tfidf %>%
  ungroup() %>%
  anti_join(train_ids, by = "sentence.id") %>%
  select(-sentence.id)

train_bow_tfidf_x = select(train_bow_tfidf,-pres_nr, -president_sona)
train_bow_tfidf_x = as.matrix(train_bow_tfidf_x)

train_bow_tfidf_y = train_bow_tfidf %>%
                      mutate(pres_nr = as.numeric(pres_nr)) %>%
                      select(pres_nr)
train_bow_tfidf_yc = to_categorical(as.numeric(as.matrix(train_bow_tfidf_y)))

test_bow_tfidf_x = select(test_bow_tfidf,-pres_nr, -president_sona)
test_bow_tfidf_x = as.matrix(test_bow_tfidf_x)

test_bow_tfidf_y = test_bow_tfidf %>%
                      mutate(pres_nr = as.numeric(pres_nr)) %>%
                      select(pres_nr)
test_bow_tfidf_yc = to_categorical(as.numeric(as.matrix(test_bow_tfidf_y)))

dim(train_bow_tfidf_x)
dim(train_bow_tfidf_y)
dim(test_bow_tfidf_x)
dim(test_bow_tfidf_y)

input_shape = ncol(train_bow_tfidf_x) # the number of destinct words
output_shape = ncol(test_bow_tfidf_yc) # the 6 presidents we want to predict

model7 = keras_model_sequential()

# define model layers
model7 %>% 
  layer_dense(units = 0.6*(input_shape + output_shape), #6517,   number of neurons in 1st hidden layer, use 2/3 of (input_shape + output_shape)
  input_shape = input_shape) %>%           # dimension of input array
  layer_activation('relu') %>%             # use a rectified linear unit as an activation function in the hidden layer
  layer_dense(units = output_shape) %>%    # adds an output layer to the network
  layer_activation('softmax')              # use softmax activation function in the output layer

# compile model
model7 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# fit model
history7 = model7 %>%
  fit(x = train_bow_tfidf_x,
      y = train_bow_tfidf_yc,
      epochs = 1, 
      batch_size = 32,
      validation_data = list(test_bow_tfidf_x, test_bow_tfidf_yc))

plot(history7)
```

Save model

```{r}
save_model_hdf5(model7, "models/FE_SONA_model7.h5") #save model to use later on
save_model_weights_hdf5(model7, "models/FE_SONA_model7_weights.h5") #save model weights to use later on
save(history7, file = "models/history7.RData")
save(pres_lookup, sona_bow_tfidf, train_ids, train_bow_tfidf, test_bow_tfidf, train_bow_tfidf_x, train_bow_tfidf_y, train_bow_tfidf_yc, test_bow_tfidf_x, test_bow_tfidf_y, test_bow_tfidf_yc, input_shape, output_shape, file = "models/NN_TFIDF.RData")
```


Compare train and test accuracy

```{r}
cat("Train classification success rate = ", mean(model7$predict_classes(train_bow_tfidf_x) == train_bow_tfidf_y), "\n")
cat("Test classification success rate = ", mean(model7$predict_classes(test_bow_tfidf_x) == test_bow_tfidf_y))
```


```{r}

## CNN tfidf commented out

# library(keras)
# set.seed(123)
# 
# sona_bow_tfidf <- sona_words_tfidf %>% 
#   ungroup() %>% 
#   select(president_sona, sentence.id, word_sona, tf_idf) %>%  # note the change, using tf-idf
#   group_by(president_sona) %>% 
#   top_n(2000) %>% 
#   ungroup() %>% 
#   spread(key = word_sona, value = tf_idf, fill = 0) 
# 
# pres_lookup = as.tibble(as.matrix(cbind(pres_nr = as.numeric(c(0,1,2,3,4,5)) ,
#               president_sona = c('deKlerk','Mandela','Mbeki','Motlanthe','Zuma','Ramaphosa'))))
# 
# sona_bow_tfidf = sona_bow_tfidf %>%
#   left_join(pres_lookup, by = "president_sona") %>%
#   select(president_sona, pres_nr, sentence.id, everything())
# 
# unique(sona_bow_tfidf$pres_nr)
# 
# train_ids <- sona_bow_tfidf %>% 
#   group_by(president_sona) %>% 
#   sample_frac(0.7) %>% 
#   ungroup() %>%
#   select(sentence.id)
# 
# train_bow_tfidf <- sona_bow_tfidf %>%
#   ungroup() %>%
#   right_join(train_ids, by = "sentence.id") %>%
#   select(-sentence.id)
# 
# unique(train_bow_tfidf$pres_nr)
# 
# test_bow_tfidf <- sona_bow_tfidf %>%
#   ungroup() %>%
#   anti_join(train_ids, by = "sentence.id") %>%
#   select(-sentence.id)
# 
# train_bow_tfidf_x = select(train_bow_tfidf,-pres_nr, -president_sona)
# train_bow_tfidf_x = as.matrix(train_bow_tfidf_x)
# 
# train_bow_tfidf_y = train_bow_tfidf %>%
#                       mutate(pres_nr = as.numeric(pres_nr)) %>%
#                       select(pres_nr)
# train_bow_tfidf_yc = to_categorical(as.numeric(as.matrix(train_bow_tfidf_y)))
# 
# test_bow_tfidf_x = select(test_bow_tfidf,-pres_nr, -president_sona)
# test_bow_tfidf_x = as.matrix(test_bow_tfidf_x)
# 
# test_bow_tfidf_y = test_bow_tfidf %>%
#                       mutate(pres_nr = as.numeric(pres_nr)) %>%
#                       select(pres_nr)
# test_bow_tfidf_yc = to_categorical(as.numeric(as.matrix(test_bow_tfidf_y)))
# 
# dim(train_bow_tfidf_x)
# dim(train_bow_tfidf_y)
# dim(test_bow_tfidf_x)
# dim(test_bow_tfidf_y)
# 
# input_shape = ncol(train_bow_tfidf_x) # the number of destinct words
# output_shape = ncol(test_bow_tfidf_yc) # the 6 presidents we want to predict
# 
# 
# 
# # max_features = ncol(train_bow_tfidf_x) #2000       # choose max_features most popular words
# # minlen = 5                # exclude sentences shorter than this
# # maxlen = 75               # longest sentence (for padding)
# # embedding_dims = 30       # number of dimensions for word embedding
# # 
# # # use Keras to tokenize the sentences
# # tokenizer = text_tokenizer(num_words = max_features)
# # fit_text_tokenizer(tokenizer, sona_sentences$sentence)
# # sequences = tokenizer$texts_to_sequences(sona_sentences$sentence)
# # 
# # # remove sentences shorter than minlen
# # seq_index = unlist(lapply(sequences, length)) > minlen
# # 
# # # exclude short sequences
# # lengthIs = function(n) function(x) length(x) > n
# # sequences = Filter(lengthIs(minlen), sequences)
# # 
# # # get y predicter
# # predicter_presidents_name_cnn = sona_sentences$president[seq_index]
# # predicter_presidents_id_cnn = (0:5)[ match(predicter_presidents_name_cnn, c("deKlerk", "Mandela", "Mbeki", "Motlanthe", "Zuma", "Ramaphosa") ) ] 
# # 
# # # build train and test set
# # train_cnn = list()
# # test_cnn = list()
# # 
# # train.index.cnn = sample(1:length(sequences),
# #                          size = 0.8 * length(sequences),
# #                          replace = F)
# # 
# # train_cnn$x = sequences[train.index.cnn]
# # test_cnn$x =  sequences[-train.index.cnn]
# # 
# # train_cnn$y = predicter_presidents_id_cnn[train.index.cnn]
# # test_cnn$y =  predicter_presidents_id_cnn[-train.index.cnn]
# # 
# # # padd shorter sequences with zeros
# # x_train_cnn = train_cnn$x %>% pad_sequences(maxlen = maxlen)
# # x_test_cnn = test_cnn$x %>% pad_sequences(maxlen = maxlen)
# # 
# # # one hot encoding
# # y_train_cnn = to_categorical(train_cnn$y)
# # y_test_cnn = to_categorical(test_cnn$y)
# # 
# # 
# # dim(x_train_cnn)
# # dim(y_train_cnn)
# # dim(x_test_cnn)
# # dim(y_test_cnn)
# 
# # initiate model
# 
# 
# 
# # model %>%
# #   layer_conv_2d(filters = 32,                      # number of convolution filters in conv layer 1
# #                 kernel_size = c(3,3),              # use 3 x 3 convolution filter in conv layer 1
# #                 input_shape = c(28, 28, 1)) %>%    # shape of input data
# #   layer_activation('relu') %>%                     # activation function in conv layer 1
# #   layer_dropout(rate = 0.20) %>%                   # apply 20% dropout after conv layer 1
# #   layer_conv_2d(filters = 64,                      # number of convolution filters in conv layer 2
# #                 kernel_size = c(3,3)) %>%          # also use 3 x 3 filter in conv layer 2
# #   layer_activation('relu') %>%                     # activation function in conv layer 2
# #   layer_max_pooling_2d(pool_size = c(2, 2)) %>%    # apply max pooling after conv layer 2
# #   layer_flatten() %>%                              # flatten output into a vector
# #   layer_dense(units = 10, activation = 'softmax')  # fully connected to output layer
# 
# 
# dim(train_bow_tfidf_x) <- c(nrow(train_bow_tfidf_x), ncol(train_bow_tfidf_x), 1)
# dim(test_bow_tfidf_x) <- c(nrow(test_bow_tfidf_x), ncol(test_bow_tfidf_x), 1) 
# # dim(x_test) <- c(nrow(x_test), 28, 28, 1)
# 
# 
# model8 = keras_model_sequential()
# 
# # define model layers
# model8 %>% 
#    # layer_dense(units = ncol(train_bow_tfidf_x)/2,                   # number of neurons in 1st hidden layer
#    #             activation = 'relu',              # use a rectified linear unit as an activation function in 1st hidden layer
#    #             input_shape = c(ncol(train_bow_tfidf_x),nrow(train_bow_tfidf_x))
#    #             ) %>%
#    # 
#    # # add some dropout
#    # layer_dropout(0.2) %>%
#    # 
#   layer_conv_1d(filters = 250,
#     kernel_size = 3,
#     activation = 'relu',
#     strides = 1,
#     input_shape = c(ncol(train_bow_tfidf_x),1)
#     ) %>%
# 
#    # # add some dropout
#    # layer_dropout(0.2) %>%
#    
#   # # convolutional layer
#   # layer_conv_1d(
#   #   filters = 250,
#   #   kernel_size = 3,
#   #   # padding = 'valid',  # "valid" means no padding, as we did it already
#   #   activation = 'relu',
#   #   strides = 1
#   # ) %>%
#   # 
#   #  layer_global_max_pooling_1d() %>%
#   # 
#   #  layer_dense(units = 128,
#   #              activation = 'relu') %>%
#   #  layer_dropout(0.2) %>%
# 
#   layer_dense(units = 6) %>%
#   layer_activation('softmax')
# 
# # compile model
# model8 %>% compile(
#   optimizer = 'adam',
#   loss = 'categorical_crossentropy',
#   metrics = c('accuracy')
# )
# 
# # fit model
# model8 %>%
#   fit(x = train_bow_tfidf_x,   # for the x input we exclude columns 1=sentence.id, and 9771=predicter.presidents.name
#       y = train_bow_tfidf_y,
#       class_weight = list("0"=27.3, "1"=1.6, "2"=1.1, "3"=10, "4"=1, "5"=11.2),
#       epochs = 5,
#       batch_size = 32,
#       validation_data = list(test_bow_tfidf_x, test_bow_tfidf_y)) %>%
#   plot()
```


## Feed-forward neural network
  
We can now create a basic feed-forward neural network using the keras package.

The following architecture has been selected for the model:

* A single hidden layer with 640 neurons.
* Our activation function for the hidden layer will be ReLU (rectified linear unit where any negative values will be made zero). This is safe to use as we do not have any values below zero, so we do not need to be concerned regarding 'vanishing' gradients for negative values. 
* We will use the softmax activation function for our output to predict a probability for each of the 6 president classes.
* The selected optimizer is Adam (the best stochastic optimizer at present with a default learning rate of 0.001).
* Our loss function will be categorical cross entropy as we have a multi-class problem to solve.
* The accuracy metric will be used to access the performance of our model. We will assess the accuracy of our model by using a train and test dataset. Train data will be 80% of the sentences. Test data will be the remaining 20%.
* The maximum number of epochs will be 20, but we enable early stopping in case the validation loss starts to increase to avoid overfitting.

```{r}
library(keras)
set.seed(123)
use_session_with_seed(seed = 123, disable_gpu = F, disable_parallel_cpu = F, quiet = T) # set seed in Keras and TensorFlow

# scale data
# sona_matrix = scale(sona_matrix) # NOTE: scaling did not improve model performance

# select training index
train.index = sample(1:nrow(sona_matrix), size = nrow(sona_matrix) * 0.8, replace = F)

# initiate model
model1 = keras_model_sequential()

input_shape = ncol(sona_matrix) # the number of destinct words
output_shape = 6 # the 6 presidents we want to predict

# convert presidents into numbers
# 0 = deKlerk
# 1 = Mandela
# 2 = Mbeki
# 3 = Motlanthe
# 4 = Zuma
# 5 = Ramaphosa
predicter_presidents_id = (0:5)[ match(predicter_presidents_name, c("deKlerk", "Mandela", "Mbeki", "Motlanthe", "Zuma", "Ramaphosa") ) ] 

x_train = sona_matrix[train.index, ]
x_test = sona_matrix[-train.index, ]

y_train = predicter_presidents_id[train.index]
y_test = predicter_presidents_id[-train.index]

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# define model layers
model1 %>% 
  layer_dense(units = 640,                 # number of neurons in the hidden layer
  input_shape = input_shape) %>%           # dimension of input array
  layer_activation('relu') %>%             # use a rectified linear unit as an activation function in the hidden layer
  layer_dense(units = output_shape) %>%    # adds an output layer to the network
  layer_activation('softmax')              # use softmax activation function in the output layer

# compile model
model1 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# fit model
history1 = model1 %>%
  fit(x = x_train,
      y = y_train,
      epochs = 20,
      batch_size = 32,
      callbacks = list(callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 3, verbose = 0)),
      verbose = 0,
      validation_data = list(x_test, y_test))

plot(history1)
```

Note that the warning message above the graph is due to the early stopping setting we used. Our epochs were set to 20, but after 4 epochs the validation loss kept increasing, so training was terminated.

Save model

```{r}
save_model_hdf5(model1, "models/Waldo_SONA_model1.h5") #save model to use later on
save_model_weights_hdf5(model1, "models/Waldo_SONA_model1_weights.h5") #save model weights to use later on
save(history1, file = "models/history1.RData")
```

Load model

```{r}
model1 = load_model_hdf5("models/Waldo_SONA_model1.h5")
load_model_weights_hdf5(model1, "models/Waldo_SONA_model1_weights.h5")
load("models/history1.RData")
```


Compare train and test accuracy

```{r}
cat("Train classification success rate = ", mean(model1$predict_classes(x_train) == predicter_presidents_id[train.index]), "\n")
cat("Test classification success rate = ", mean(model1$predict_classes(x_test) == predicter_presidents_id[-train.index]))
```


From the above result we can see that the model is overfitting as the train accuracy is more than double that of the test accuracy.
A random guess would be 
* 16.67% if random 1 out of 6 presidents are guessed, or 
* if we repeatedly guess Zuma based on most sentences (sentence distribution) a chance of 36.3% to be correct.
We see that our test accuracy is significantly better than a random guess.   
For this first model we have only used 640 neurons in a single hidden layer, let's expand the architecture to see if we can improve on this model.

Our next model will use the following architecture:

* Adding a 1st hidden layer with 6517 neurons, where this number has been determined by taking 2/3 of (input variables + output variables).
* Applying a 30% random dropout rate on the 1st layer for each epoch.
* Adding a 2nd hidden layer with 3258 neurons (half the size of the 1st hidden layer).
* Applying a 20% random dropout rate on the 2nd layer for each epoch.
* Our activation function for the hidden layers will be ReLU (rectified linear unit where any negative values will be made zero).
* We will use the softmax activation function for our output to predict a probability for each of the 6 presidents.
* The selected optimizer is Adam (the best stochastic optimizer at present with a default learning rate of 0.001).
* Our loss function will be categorical cross entropy as we have a multi-class problem to solve.
* The accuracy metric will be used to access the performance of our model.

```{r}
# initiate model
model2 = keras_model_sequential()

# define model layers
model2 %>% 
  layer_dense(units = 6517,                     # number of neurons in 1st hidden layer, use 2/3 of (input_shape + output_shape)
              activation = 'relu',              # use a rectified linear unit as an activation function in 1st hidden layer
              input_shape = input_shape) %>%    # dimension of input array
  layer_dropout(rate = 0.3) %>%                 # add dropout with rate 30%
  
  layer_dense(units = 3258,                     # number of neurons in 2nd hidden layer, use 1/2 of 1st hidden layer
              activation = 'relu') %>%          # use a rectified linear unit as an activation function in 2nd hidden layer
  layer_dropout(rate = 0.2) %>%                 # add dropout with rate 20%
  
  layer_dense(units = output_shape) %>%         # adds an output layer to the network
  layer_activation('softmax')                   # use softmax activation function in the output layer

# compile model
model2 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# fit model
history2 = model2 %>%
  fit(x = x_train,
      y = y_train,
      epochs = 20,
      batch_size = 32,
      callbacks = list(callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 3, verbose = 0)),
      verbose = 0,
      validation_data = list(x_test, y_test))

plot(history2)
```

```{r}
cat("Train classification success rate = ", mean(model2$predict_classes(x_train) == predicter_presidents_id[train.index]), "\n")
cat("Test classification success rate = ", mean(model2$predict_classes(x_test) == predicter_presidents_id[-train.index]))
```


We can see our test accuracy improved slightly from 48.2% to 49%, but the model is still overfitting.

We can also look at the confusion matrix:

```{r}
table(predicted = model2$predict_classes(x_test), actual = predicter_presidents_id[-train.index])
```

This table shows that, for:

* deKlerk (ID = 0), not a single prediction was correct. This can be expected as we had the least number of words for deKlerk (only 1% of all words were assosiated with him).
* Ramaphosa (ID = 5), only 1 predictions were correct. This is also expected as only 2.9% of all words were assosiated with him.
* Zuma (ID = 4), where we had most predictions correct and we also had 30.9% of all words assosiated with him.
* Mbeki (ID = 2), for whom we had the highest word percentage assosiation (39.1%) and where more than half of the predictions were correct.

Save model

```{r}
save_model_hdf5(model2, "models/Waldo_SONA_model2.h5") #save model to use later on
save_model_weights_hdf5(model2, "models/Waldo_SONA_model2_weights.h5") #save model weights to use later on
save(history2, file = "models/history2.RData")
```

Load model

```{r}
model2 = load_model_hdf5("models/Waldo_SONA_model2.h5")
load_model_weights_hdf5(model2, "models/Waldo_SONA_model2_weights.h5")
load(file = "models/history2.RData")
```

Let's see if we can get an improvement, by correcting the class imbalance. We will use the weight adjustments which we have already calculated for grouping sentences per president.

```{r}
# initiate model
model3 = keras_model_sequential()

# define model layers
model3 %>% 
  layer_dense(units = 6517,                     # number of neurons in 1st hidden layer, use 2/30 of (input_shape + output_shape)
              activation = 'relu',              # use a rectified linear unit as an activation function in 1st hidden layer
              input_shape = input_shape) %>%    # dimension of input array
  layer_dropout(rate = 0.3) %>%                 # add dropout with rate 30%
  
  layer_dense(units = 3258,                     # number of neurons in 2nd hidden layer, use 1/2 of 1st hidden layer
              activation = 'relu') %>%          # use a rectified linear unit as an activation function in 2nd hidden layer
  layer_dropout(rate = 0.2) %>%                 # add dropout with rate 20%
  
  layer_dense(units = output_shape) %>%         # adds an output layer to the network
  layer_activation('softmax')                   # use softmax activation function in the output layer

# compile model
model3 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# fit model
history3 = model3 %>%
  fit(x = x_train,
      y = y_train,
      class_weight = list("0"=27.3, "1"=1.6, "2"=1.1, "3"=10, "4"=1, "5"=11.2), # balance classes
      epochs = 20,
      batch_size = 32,
      callbacks = list(callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 1, verbose = 0)),
      verbose = 0,
      validation_data = list(x_test, y_test))

plot(history3)
```

```{r}
cat("Train classification success rate = ", mean(model3$predict_classes(x_train) == predicter_presidents_id[train.index]), "\n")
cat("Test classification success rate = ", mean(model3$predict_classes(x_test) == predicter_presidents_id[-train.index]))
```

Note our overall performance has not improved and sit on 45%. It is worst than our first model.

Looking at the confusion matrix we can however see the following improvement:

* deKlerk (ID = 0), we now have 6 correct prediction compared with 0 before.
* Ramaphosa (ID = 5), we now also have 6 correct prediction compared with 1 before.

This means that even though our overall performance have decreased, we at least have better performance for the classes which were undersampled.

```{r}
table(predicted = model3$predict_classes(x_test), actual = predicter_presidents_id[-train.index])
```

Save model

```{r}
save_model_hdf5(model3, "models/Waldo_SONA_model3.h5") #save model to use later on
save_model_weights_hdf5(model3, "models/Waldo_SONA_model3_weights.h5") #save model weights to use later on
save(history3, file = "models/history3.RData")
```

Load model

```{r}
model3 = load_model_hdf5("models/Waldo_SONA_model3.h5")
load_model_weights_hdf5(model3, "models/Waldo_SONA_model3_weights.h5")
load(file = "models/history3.RData")
```

Sentence summary with stop words:

```{r}
# get sentence summary (with stop words)
sona_sentance_summary_with_stop_words = sona_sentences %>%
  unnest_tokens(word, sentence, token = "words") %>%
  group_by(sentence.id, word) %>%
  summarise(count = n()) %>%
  group_by(sentence.id) %>%
  summarise(words = sum(count)) %>%
  arrange(desc(words))

sona_sentance_summary_with_stop_words %>%
  summarise(Total_sentance = n(),
            Longest_sentance = max(words),
            Average_sentance = mean(words),
            Shortest_sentance = min(words)) %>%
  select(Total_sentance, Longest_sentance, Average_sentance, Shortest_sentance)
```

Sentence summary without stop words:

```{r}
# get sentance summary (without stop words)
sona_sentance_summary_without_stop_words = sona_words %>%
  group_by(sentence.id) %>%
  summarise(words = sum(count)) %>%
  arrange(desc(words))

sona_sentance_summary_without_stop_words %>%
  summarise(Total_sentance = n(),
            Longest_sentance = max(words),
            Average_sentance = mean(words),
            Shortest_sentance = min(words)) %>%
  select(Total_sentance, Longest_sentance, Average_sentance, Shortest_sentance)
```


## CNN with embeddings

Another approach we can consider is using a Convolutional Neural Network (CNN) which is good when dealing with data in which the input features exhibit some kind of spatial or temporal ordering. CNNs are designed to exploit the structure of ordered data. Their fundamental building block is the convolution filter, which is a small matrix (normally 3x3), which is applied to the data to help extract hidden features.
We can add embeddings to the CNN, which is simmalar to the latent variables concept we used for building a recommender system.

For this model we will not use the previous sona_matrix object. Instead we will use Keras to tokenize the sentences into sequences will be used as input for the embeddings to the CNN.

We first need to look at a few things to determine the best parameters for the CNN embeddings.

We will start with checking sentence lengths. In other words what is the average number of words per sentence and what is the maximum length of sentences we should consider to represent 98% of the data?
Note that for these sentences all stop words and punctuation have been removed. We are using the sona_words object and joining it to sona_sentences to achieve this.

```{r}
sentence_word_counts = sona_sentences %>%
  left_join(sona_words, by = "sentence.id") %>%
  group_by(sentence.id) %>%
  summarise(count = sum(count, na.rm = T)) %>%
  select(sentence.id, count) %>%
  arrange(count)

#plot(x = 1:nrow(sentence_word_counts), y = sentence_word_counts$count)
hist(sentence_word_counts$count, col = "lightgrey", main = "Histogram of sentence word counts", xlab = "Total words in sentence", xlim = c(0,130))

```

```{r}
cat("average number of words per sentence is", mean(sentence_word_counts$count), "\n")
cat("maximum length of sentences which represent 98% of data is", quantile(sentence_word_counts$count, 0.98), "\n\n")

cat("# sentence with  3 or less words is", sum(sentence_word_counts$count <= 3), "\n")
cat("# sentence with  4 or less words is", sum(sentence_word_counts$count <= 4), "\n")
cat("# sentence with  5 or less words is", sum(sentence_word_counts$count <= 5), "\n")
cat("# sentence with 10 or less words is", sum(sentence_word_counts$count <= 10), "\n")
cat("# sentence with 29 or less words is", sum(sentence_word_counts$count <= 29), "\n\n")

cat("# sentence with more than  30 words is", sum(sentence_word_counts$count > 30), "\n")
cat("# sentence with more than  50 words is", sum(sentence_word_counts$count > 50), "\n")
cat("# sentence with more than  75 words is", sum(sentence_word_counts$count > 75), "\n")
cat("# sentence with more than 100 words is", sum(sentence_word_counts$count > 100), "\n")
cat("# sentence with more than 125 words is", sum(sentence_word_counts$count > 125), "\n")
cat("# sentence with more than 150 words is", sum(sentence_word_counts$count > 150))
```


Check frequency of words accross all sentences to help determine the maximum features to use. In other words, how many unique words should we keep to still represent 95% of the data?

```{r}
word_counts = sona_words %>%
  group_by(word) %>%
  summarise(count = sum(count)) %>%
  select(word, count) %>%
  arrange(count)

#plot(x = 1:nrow(word_counts), y = word_counts$count)
hist(word_counts$count, col = "lightgrey", main = "Histogram of word count occurences", xlab = "Word count", xlim=c(0,300))
```

```{r}
cat("average frequency of words are", mean(word_counts$count), "\n")
cat("# words to keep to reprecent 95% of the data is", nrow(word_counts) * 0.95, "\n\n")

cat("# words occuring once is", sum(word_counts$count == 1), "\n")
cat("# words occuring 2 or less times is", sum(word_counts$count <= 2), "\n")
cat("# words occuring 3 or less times is", sum(word_counts$count <= 3), "\n")
cat("# words occuring 4 or less times is", sum(word_counts$count <= 4), "\n")
cat("# words occuring 5 or less times is", sum(word_counts$count <= 5), "\n\n")

cat("# words used more than once is", sum(word_counts$count > 1), "\n")
cat("# words used more than   5 times is", sum(word_counts$count > 5), "\n")
cat("# words used more than  10 times is", sum(word_counts$count > 10), "\n")
cat("# words used more than  20 times is", sum(word_counts$count > 20), "\n")
cat("# words used more than  50 times is", sum(word_counts$count > 50), "\n")
cat("# words used more than 100 times is", sum(word_counts$count > 100), "\n")
cat("# words used more than 200 times is", sum(word_counts$count > 200))
```


Add a new column to sona_sentences which only contains the core of each sentence, this will be the sentence with all stop words and punctuation removed. This will also match what we have in the sona_words object, but with the added value of having it in string where the order of the words are retained.

```{r}
core_sentence = vector(length = nrow(sona_sentences))

for (i in 1:nrow(sona_sentences))
{
  core_sentence[i] = unnest_tokens(tbl = data_frame(sentence = sona_sentences[i,]$sentence),
                                    output = word,
                                    input = sentence, token = "words") %>%
    filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %>% # remove stop words
    unlist() %>% # turn list into character vector
    paste(collapse = " ") # return singe string from character vector
}

sona_sentences$core_sentence = core_sentence
rm(core_sentence)

# as a test display the original and core sentences of one entry
cat("Original sentence:\n", sona_sentences[2500,]$sentence, sep = "", "\n\n")
cat("Core sentence:\n", sona_sentences[2500,]$core_sentence, sep = "")
```


We are now ready to build our CNN model.

```{r}
max_features = 3000       # choose max_features most popular words
minlen = 2                # exclude sentences shorter than this
maxlen = 50               # longest sentence (for padding)
embedding_dims = 50       # number of dimensions for word embedding
output_shape = 6          # the 6 presidents we want to predict

# use Keras to tokenize the sentences
tokenizer = text_tokenizer(num_words = max_features)
fit_text_tokenizer(tokenizer, sona_sentences$core_sentence)
sequences = tokenizer$texts_to_sequences(sona_sentences$core_sentence)

# remove sentences shorter than minlen
seq_index = unlist(lapply(sequences, length)) > minlen

# exclude short sequences
lengthIs = function(n) function(x) length(x) > n
sequences = Filter(lengthIs(minlen), sequences)

# get y predicter
predicter_presidents_name_cnn = sona_sentences$president[seq_index]
predicter_presidents_id_cnn = (0:5)[ match(predicter_presidents_name_cnn, c("deKlerk", "Mandela", "Mbeki", "Motlanthe", "Zuma", "Ramaphosa") ) ] 

# build train and test set
train_cnn = list()
test_cnn = list()

train.index.cnn = sample(1:length(sequences),
                         size = 0.8 * length(sequences),
                         replace = F)

train_cnn$x = sequences[train.index.cnn]
test_cnn$x =  sequences[-train.index.cnn]

train_cnn$y = predicter_presidents_id_cnn[train.index.cnn]
test_cnn$y =  predicter_presidents_id_cnn[-train.index.cnn]

# padd shorter sequences with zeros
x_train_cnn = train_cnn$x %>% pad_sequences(maxlen = maxlen)
x_test_cnn = test_cnn$x %>% pad_sequences(maxlen = maxlen)

# one hot encoding
y_train_cnn = to_categorical(train_cnn$y)
y_test_cnn = to_categorical(test_cnn$y)

# initiate model
model4 = keras_model_sequential()

# define model layers
model4 %>% 
  # embedding layer maps vocab indices into embedding_dims dimensions
  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
  
  # add some dropout
  layer_dropout(0.2) %>%
  
  # convolutional layer
  layer_conv_1d(
    filters = 500,
    kernel_size = 3, 
    padding = 'valid',  # "valid" means no padding, as we did it already
    activation = 'relu', 
    strides = 1) %>%

  layer_global_max_pooling_1d() %>%
  
  layer_dense(units = 128,
              activation = 'relu') %>%
  layer_dropout(0.2) %>%

  layer_dense(units = output_shape) %>%
  layer_activation('softmax')

# compile model
model4 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# fit model
history4 = model4 %>%
  fit(x = x_train_cnn,
      y = y_train_cnn,
      epochs = 20,
      batch_size = 16,
      callbacks = list(callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 1)),
      verbose = 0,
      validation_data = list(x_test_cnn, y_test_cnn))

plot(history4)
```

Save model

```{r}
save_model_hdf5(model4, "models/Waldo_SONA_model4.h5") #save model to use later on
save_model_weights_hdf5(model4, "models/Waldo_SONA_model4_weights.h5") #save model weights to use later on
save(history4, file = "models/history4.RData")
```

Load model

```{r}
model4 = load_model_hdf5("models/Waldo_SONA_model4.h5")
load_model_weights_hdf5(model4, "models/Waldo_SONA_model4_weights.h5")
load(file = "models/history4.RData")
```

```{r}
cat("Train classification success rate = ", mean(model4$predict_classes(x_train_cnn) == train_cnn$y), "\n")
cat("Test classification success rate = ", mean(model4$predict_classes(x_test_cnn) == test_cnn$y))
```

Model 4 is a good improvement on the previous models. We now have an accuracy of 59.1% where the previous best was 49%

We have run 60 previous versions of this model to help determine what the best parameter setting should be. There were 20 tests, where each test was run 3 times and the best test accuracy was recorded. It is difficult to exactly repreduce the keras results from within R as Phyton will set its own random values with a different seed. The following table provide a summary of these tests. Note that parameter values are only displayed when a change to it was made. If no value is displayed in one of the parameter columns then the previous value was used for that specific test.

```{r}
cnn_parameter_tests = read.csv("cnn_parameter_tests.csv")
data.frame(cnn_parameter_tests)
```

We can again try and see if we can improve on this model by correcting the class imbalances. The previous class weights calculated on the sona_sentences object which was use for the sona_matrix cannot be used in this case as we have how excluded short sentences. The following table display the new weights which we should consider for our CNN model:

```{r}
cnn_class_balance = read.csv("cnn_class_balance.csv")
data.frame(cnn_class_balance)
```


We can try 3 different approaches:

1. Use the class_weight attribute when fitting the keras model, and set the weights according to how additional sampling should take place. E.g. Zuma (ID=4) have the most sentences and do not require any correction, therefor class weight is 1. deKlerk (ID=0) on the other hand has the least number of sentences and require the biggest correction, which is a class weight of 26.8. The "Class.Balance"" attribute above list these weights.
2. Also use the keras class_weight attribute, but in this case set the class weights to the partial percentage which the class is contributing to. E.g. for Zuma 36.1% and for deKlerk 1.3%. The "sentence.percentage"" attribute above list these weights. Note that this approach is less common than the first, but still referred to in literature.
3. Correcting the class imbalances by adding extra samples for the classes which require adjustments. E.g. for Zuma 0 additional samples will be needed, but for deKlerk we will need to add an extra 2294 samples. The "Number.of.additional.samples.needed" attribute above list these extra number of samples required.


Approach 1:

```{r}
# fit model
history4.1 = model4 %>%
  fit(x = x_train_cnn,
      y = y_train_cnn,
      class_weight = list("0"=26.8, "1"=1.5, "2"=1.0, "3"=9.6, "4"=1.0, "5"=10.7),
      epochs = 20,
      batch_size = 16,
      callbacks = list(callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 1)),
      verbose = 0,
      validation_data = list(x_test_cnn, y_test_cnn))

plot(history4.1)
```

```{r}
cat("Train classification success rate = ", mean(model4$predict_classes(x_train_cnn) == train_cnn$y), "\n")
cat("Test classification success rate = ", mean(model4$predict_classes(x_test_cnn) == test_cnn$y))
```

```{r}
table(predicted = model4$predict_classes(x_test_cnn), actual = test_cnn$y)
```


Approach 2:

```{r}
# fit model
history4.2 = model4 %>%
  fit(x = x_train_cnn,
      y = y_train_cnn,
      class_weight = list("0"=1.3, "1"=22.6, "2"=33.1, "3"=3.6, "4"=36.1, "5"=3.2),
      epochs = 20,
      batch_size = 16,
      callbacks = list(callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 1)),
      verbose = 0,
      validation_data = list(x_test_cnn, y_test_cnn))

plot(history4.2)
```

```{r}
cat("Train classification success rate = ", mean(model4$predict_classes(x_train_cnn) == train_cnn$y), "\n")
cat("Test classification success rate = ", mean(model4$predict_classes(x_test_cnn) == test_cnn$y))
```

```{r}
table(predicted = model4$predict_classes(x_test_cnn), actual = test_cnn$y)
```


Approach 3:

```{r}
# build train and test set
train_cnn = list()
test_cnn = list()

train.index.main = train.index.cnn # save the original training indexes

# add indexes for correction of class imbalance
# add extra 2294 samples for deKlerk to correct class imbalances
train.index.cnn = c(train.index.main,
                    sample(which(predicter_presidents_name_cnn[train.index.main] == "deKlerk"),
                           size = 2294,
                           replace = T))
# add extra 836 samples for Mandela to correct class imbalances
train.index.cnn = c(train.index.cnn,
                    sample(which(predicter_presidents_name_cnn[train.index.main] == "Mandela"),
                           size = 836,
                           replace = T))
# add extra 95 samples for Mbeki to correct class imbalances
train.index.cnn = c(train.index.cnn,
                    sample(which(predicter_presidents_name_cnn[train.index.main] == "Mbeki"),
                           size = 95,
                           replace = T))
# add extra 2135 samples for Motlanthe to correct class imbalances
train.index.cnn = c(train.index.cnn,
                    sample(which(predicter_presidents_name_cnn[train.index.main] == "Motlanthe"),
                           size = 2135,
                           replace = T))
# add extra 2160 samples for Ramaphosa to correct class imbalances
train.index.cnn = c(train.index.cnn,
                    sample(which(predicter_presidents_name_cnn[train.index.main] == "Ramaphosa"),
                           size = 2160,
                           replace = T))

# follow usual steps from here
train_cnn$x = sequences[train.index.cnn]
test_cnn$x =  sequences[-train.index.cnn]

train_cnn$y = predicter_presidents_id_cnn[train.index.cnn]
test_cnn$y =  predicter_presidents_id_cnn[-train.index.cnn]

# padd shorter sequences with zeros
x_train_cnn = train_cnn$x %>% pad_sequences(maxlen = maxlen)
x_test_cnn = test_cnn$x %>% pad_sequences(maxlen = maxlen)

# one hot encoding
y_train_cnn = to_categorical(train_cnn$y)
y_test_cnn = to_categorical(test_cnn$y)

# initiate model
model4.3 = keras_model_sequential()

# define model layers
model4.3 %>% 
  # embedding layer maps vocab indices into embedding_dims dimensions
  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
  
  # add some dropout
  layer_dropout(0.2) %>%
  
  # convolutional layer
  layer_conv_1d(
    filters = 500,
    kernel_size = 3, 
    padding = 'valid',  # "valid" means no padding, as we did it already
    activation = 'relu', 
    strides = 1) %>%

  layer_global_max_pooling_1d() %>%
  
  layer_dense(units = 128,
              activation = 'relu') %>%
  layer_dropout(0.2) %>%

  layer_dense(units = output_shape) %>%
  layer_activation('softmax')

# compile model
model4.3 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# fit model
history4.3 = model4.3 %>%
  fit(x = x_train_cnn,
      y = y_train_cnn,
      epochs = 20,
      batch_size = 16,
      callbacks = list(callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 1)),
      verbose = 0,
      validation_data = list(x_test_cnn, y_test_cnn))

plot(history4.3)
```

Save model

```{r}
save_model_hdf5(model4.3, "models/Waldo_SONA_model4.3.h5") #save model to use later on
save_model_weights_hdf5(model4.3, "models/Waldo_SONA_model4.3_weights.h5") #save model weights to use later on
save(history4.1, file = "models/history4.1.RData")
save(history4.2, file = "models/history4.2.RData")
save(history4.3, file = "models/history4.3.RData")
```

```{r}
cat("Train classification success rate = ", mean(model4.3$predict_classes(x_train_cnn) == train_cnn$y), "\n")
cat("Test classification success rate = ", mean(model4.3$predict_classes(x_test_cnn) == test_cnn$y))
```

```{r}
table(predicted = model4.3$predict_classes(x_test_cnn), actual = test_cnn$y)
```

We can compare the approaches either on accuracy per president or average accuracy.
When using accuracy per president, then approach 1 is better.
When using average accuracy, then approach 2 is better.


## Expanding on CNN with embeddings

We can try and make our CNN model better by adding more convolutional layers. For the next model we will use the same basis as for model 4 by keeping the same max_features, minlen, maxlen and embedding_dims.

```{r}
# build train and test set
train_cnn = list()
test_cnn = list()

train.index.cnn = train.index.main # use same training index as for the first CNN model

train_cnn$x = sequences[train.index.cnn]
test_cnn$x =  sequences[-train.index.cnn]

train_cnn$y = predicter_presidents_id_cnn[train.index.cnn]
test_cnn$y =  predicter_presidents_id_cnn[-train.index.cnn]

# padd shorter sequences with zeros
x_train_cnn = train_cnn$x %>% pad_sequences(maxlen = maxlen)
x_test_cnn = test_cnn$x %>% pad_sequences(maxlen = maxlen)

# one hot encoding
y_train_cnn = to_categorical(train_cnn$y)
y_test_cnn = to_categorical(test_cnn$y)

# initiate model
model5 = keras_model_sequential()

# define model layers
model5 %>% 
  # embedding layer maps vocab indices into embedding_dims dimensions
  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
  
  # add some dropout
  layer_dropout(0.2) %>%
  
  # 1st convolutional layer
  layer_conv_1d(
    filters = 500,
    kernel_size = 2, 
    padding = 'valid',  # "valid" means no padding, as we did it already
    activation = 'relu', 
    strides = 1) %>%

  # 2nd convolutional layer
  layer_conv_1d(
    filters = 500,
    kernel_size = 3, 
    padding = 'valid',  # "valid" means no padding, as we did it already
    activation = 'relu', 
    strides = 1) %>%

  # 3rd convolutional layer
  layer_conv_1d(
    filters = 500,
    kernel_size = 4, 
    padding = 'valid',  # "valid" means no padding, as we did it already
    activation = 'relu', 
    strides = 1) %>%

  layer_global_max_pooling_1d() %>%
  
  layer_dense(units = 128,
              activation = 'relu') %>%
  layer_dropout(0.2) %>%

  layer_dense(units = output_shape) %>%
  layer_activation('softmax')

# compile model
model5 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# fit model
history5 = model5 %>%
  fit(x = x_train_cnn,
      y = y_train_cnn,
      epochs = 20,
      batch_size = 16,
      callbacks = list(callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 3)), # increase patience from 1 to 3
      verbose = 0,
      validation_data = list(x_test_cnn, y_test_cnn))

plot(history5)
```

Save model

```{r}
save_model_hdf5(model5, "models/Waldo_SONA_model5.h5") #save model to use later on
save_model_weights_hdf5(model5, "models/Waldo_SONA_model5_weights.h5") #save model weights to use later on
save(history5, file = "models/history5.RData")
```

Load model

```{r}
model5 = load_model_hdf5("models/Waldo_SONA_model5.h5")
load_model_weights_hdf5(model5, "models/Waldo_SONA_model5_weights.h5")
load("models/history5.RData")
```

```{r}
cat("Train classification success rate = ", mean(model5$predict_classes(x_train_cnn) == train_cnn$y), "\n")
cat("Test classification success rate = ", mean(model5$predict_classes(x_test_cnn) == test_cnn$y))
```

This model only achieved an accuracy of 57.4% which is not better than our first CNN model which achieved 59.1%. It seems that a more general approach worked better than to add to many extra layers.


## RNN

We can also look at a Recurrent Neural Network (RNN) which is similar to a CNN, where the output depends not only on the present inputs but also on the previous steps neuron state. We can also use Keras to build a RNN and use the Long Short-Term Memory (LSTM) layer.

```{r}
# initiate model
model6 = keras_model_sequential()

# define model layers
model6 %>% 
  # embedding layer maps vocab indices into embedding_dims dimensions
  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
  
  # add some dropout
  layer_dropout(0.2) %>%
  
  # add long short-term memory layer
  layer_lstm(128, input_shape = c(maxlen, ncol(x_train_cnn))) %>%

  layer_dense(units = 128,
              activation = 'relu') %>%
  layer_dropout(0.2) %>%

  layer_dense(units = output_shape) %>%
  layer_activation('softmax')

# compile model
model6 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# fit model
history6 = model6 %>%
  fit(x = x_train_cnn,
      y = y_train_cnn,
      #class_weight = list("0"=26.8, "1"=1.5, "2"=1.0, "3"=9.6, "4"=1.0, "5"=10.7),
      epochs = 20,
      batch_size = 16,
      callbacks = list(callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 1)),
      verbose = 0,
      validation_data = list(x_test_cnn, y_test_cnn))

plot(history6)
```

Save model

```{r}
save_model_hdf5(model6, "models/Waldo_SONA_model6.h5") #save model to use later on
save_model_weights_hdf5(model6, "models/Waldo_SONA_model6_weights.h5") #save model weights to use later on
save(history6, file = "models/history6.RData")
```

Load model

```{r}
model6 = load_model_hdf5("models/Waldo_SONA_model6.h5")
load_model_weights_hdf5(model6, "models/Waldo_SONA_model6_weights.h5")
load("models/history6.RData")
```

```{r}
cat("Train classification success rate = ", mean(model6$predict_classes(x_train_cnn) == train_cnn$y), "\n")
cat("Test classification success rate = ", mean(model6$predict_classes(x_test_cnn) == test_cnn$y))
```

The RNN is fairly simple model and have a better accuracy when compared to our previous models (59.2% compared to 59.1%).


## Make some predictions

Let's look at some test predictions by selecting 5 random sentences from our validation data set.

```{r}
# select 5 random test sentences
test_index = (1:length(sequences))[-train.index.main] # retrieve the validation indexes as positions in the sequences data set
test_samples = sample(test_index, , size = 5, replace = F) # select 5 random test sentences

j = 0
for (i in test_samples)
{
  j = j +1
  cat("TEST", j, "\n")
  cat("======", "\n\n")
  cat("Original sentence:\n", sona_sentences[i,]$sentence, sep = "", "\n\n")
  cat("Core sentence:\n", sona_sentences[i,]$core_sentence, sep = "", "\n\n")
  cat("Actual president:", sona_sentences[i,]$president, "\n")
  cat("Predicted president:", c("deKlerk", "Mandela", "Mbeki", "Motlanthe", "Zuma", "Ramaphosa")[ match(model6$predict_classes(matrix(x_test_cnn[which(test_index == i),], nrow = 1, ncol = 50)), (0:5))], "\n\n")
}
```

## Conclusion

The aim for this dataset was to extract interesting features in the text. This was obtained by tokenising on words, using bag of words, TF-IDF frequencies and text wrangling like removing stop words and looking at stem words. As expected, many of the words used in SONAs are repeated, but despite this, the topics found for each president (or combination thereof) did differ slightly and we could even see 'handover' from one president to another in some cases (e.g. Mandela and Mbeki used topic 3.) Sentiment analysis also showed us, interestingly, that the last speech of each president was below the average sentiment score. Using negation also increased the accuracy of the sentiment analysis score, where we could see that for example de Klerk's speech scored higher (more positive) when using negation than without it. 
 
To predict which president said a sentence, the training data was split into a train and test set. Taking a random guess for our dataset of six presidents, given a sentence, one has a chance of 16.67% (1 out of 6) to correctly guess the president who used the sentence in his SONA. A smarter guess would be to look at the distribution of sentences, then guessing always Zuma, would give an accuracy of 36.3%. So the aim with our neural networks was to beat these chances. This was achieved... 

##Group contributions  
The group had 3 members, collaborating on a shared repository on github (see https://github.com/WaldoLeonhardt/DS-for-Industry for details.) Waldo's file was used as a base, then Marike and Francois added contributions with pull and push requests. 

|Name               | Contribution                                                           |  
|-------------------|------------------------------------------------------------------------|
|All                | Data wrangling, basic Feed-forward neural net, review of report        |
|Marike du Plessis  | Sentiment analysis, Topic analysis and Worcloud                        |
|Waldo Leonhardt    | Bag of words, Bigrams with negation, TF-IDF for feed-forward neural net|
|Francois Evert     | Feed-forward neural net, CNN and RNN with hyperparameter tuning        |
Table showing group contributions




